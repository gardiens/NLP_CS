{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5503ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_data(file):\n",
    "    polarity=[]\n",
    "    Aspect_Category=[]\n",
    "    Target_term=[]\n",
    "    Character_offset=[]\n",
    "    Sentence=[]\n",
    "    polarity_to_label={\n",
    "        \"positive\":0,\n",
    "        \"negative\":1,\n",
    "        \"neutral\":2,\n",
    "    }\n",
    "    labels=[]\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "\n",
    "            # split by space and remove the \\t \n",
    "            tokens=line.split(\"\\t\") \n",
    "            polarity.append(tokens[0])\n",
    "            Aspect_Category.append(tokens[1])\n",
    "            Target_term.append(tokens[2])\n",
    "            Character_offset.append(tokens[3])\n",
    "            assert len(tokens[4:])==1,\"sentence should be one token,got \"+str(len(tokens[4:]))\n",
    "            Sentence.append(str(tokens[4:][0]))\n",
    "            labels.append(polarity_to_label[tokens[0]])\n",
    "    ds_train=pd.DataFrame({\"polarity\":polarity,\n",
    "                        \"Aspect_Category\":Aspect_Category,\n",
    "                        \"Target_term\":Target_term,\n",
    "                        \"Character_offset\":Character_offset,\n",
    "\n",
    "                        \"labels\":labels,\n",
    "                        \"Sentence\":Sentence})\n",
    "\n",
    "    ds_train = Dataset.from_pandas(ds_train)\n",
    "    return ds_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49dca9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = load_data(\"data/traindata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2d0f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val = load_data(\"data/devdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5f4eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['polarity', 'Aspect_Category', 'Target_term', 'Character_offset', 'labels', 'Sentence'],\n",
       "    num_rows: 1503\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f25c9e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'polarity': 'positive',\n",
       " 'Aspect_Category': 'AMBIENCE#GENERAL',\n",
       " 'Target_term': 'seating',\n",
       " 'Character_offset': '18:25',\n",
       " 'labels': 0,\n",
       " 'Sentence': \"short and sweet – seating is great:it's romantic,cozy and private.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3301ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"FacebookAI/roberta-large\"\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pretrained_model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16cc719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08288d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = tokenizer(ds_train[\"Sentence\"],\n",
    "                            truncation=True,\n",
    "                            padding=False,\n",
    "                            add_special_tokens=True,\n",
    "                            return_tensors=None,\n",
    "                            return_offsets_mapping=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b86997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c73b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>short and sweet – seating is great:it's romantic,cozy and private.</s>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(X_train_encoded['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92348fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'polarity': 'positive',\n",
       " 'Aspect_Category': 'AMBIENCE#GENERAL',\n",
       " 'Target_term': 'seating',\n",
       " 'Character_offset': '18:25',\n",
       " 'labels': 0,\n",
       " 'Sentence': \"short and sweet – seating is great:it's romantic,cozy and private.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06421ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0],\n",
      "        [ 0,  5],\n",
      "        [ 6,  9],\n",
      "        [10, 15],\n",
      "        [16, 17],\n",
      "        [18, 25],\n",
      "        [26, 28],\n",
      "        [29, 34],\n",
      "        [34, 35],\n",
      "        [35, 37],\n",
      "        [37, 39],\n",
      "        [40, 48],\n",
      "        [48, 49],\n",
      "        [49, 51],\n",
      "        [51, 53],\n",
      "        [54, 57],\n",
      "        [58, 65],\n",
      "        [65, 66],\n",
      "        [ 0,  0]])\n"
     ]
    }
   ],
   "source": [
    "sentence = \"short and sweet – seating is great:it's romantic,cozy and private.\"\n",
    "target_term = \"seating\"\n",
    "target_span = (18, 25)\n",
    "\n",
    "encoding = tokenizer(\n",
    "    sentence,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "offsets = encoding['offset_mapping'][0]\n",
    "print(offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3594c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token_indices = [\n",
    "    i for i, (start, end) in enumerate(offsets)\n",
    "    if start >= target_span[0] and end <= target_span[1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aebd41a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel\n",
    "model = RobertaModel.from_pretrained(\"roberta-large\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_state = outputs.last_hidden_state  # shape: (1, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe18f6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd82f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_hidden_states = last_hidden_state[0, target_token_indices, :]  # shape: (num_tokens, hidden_size)\n",
    "pooled = target_hidden_states.mean(dim=0, keepdim=True)  # shape: (hidden_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47118117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66681261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e13a0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class AspectSentimentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_labels=1):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-large\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_labels),\n",
    "        )\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Then unfreeze the pooler\n",
    "        for param in self.roberta.pooler.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, target_token_indices_batch):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # (batch, seq_len, hidden)\n",
    "        \n",
    "        pooled = []\n",
    "        for i, indices in enumerate(target_token_indices_batch):\n",
    "            token_embeddings = last_hidden_state[i, indices, :]  # (num_tokens, hidden)\n",
    "            pooled.append(token_embeddings.mean(dim=0))\n",
    "        \n",
    "        pooled = torch.stack(pooled)  # (batch_size, hidden_size)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad580c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1503 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1503/1503 [00:00<00:00, 5455.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    sentence = example['Sentence']\n",
    "    target = example['Target_term']\n",
    "    char_offset = tuple(map(int, example['Character_offset'].split(':')))\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "    offsets = encoding['offset_mapping']\n",
    "    target_indices = [i for i, (start, end) in enumerate(offsets) if start >= char_offset[0] and end <= char_offset[1]]\n",
    "\n",
    "    # Add tokenized input + token indices\n",
    "    example['input_ids'] = encoding['input_ids']\n",
    "    example['attention_mask'] = encoding['attention_mask']\n",
    "    example['target_token_indices'] = target_indices\n",
    "    return example\n",
    "\n",
    "ds_train = ds_train.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a221b670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'polarity': 'positive',\n",
       " 'Aspect_Category': 'AMBIENCE#GENERAL',\n",
       " 'Target_term': 'seating',\n",
       " 'Character_offset': '18:25',\n",
       " 'labels': 0,\n",
       " 'Sentence': \"short and sweet – seating is great:it's romantic,cozy and private.\",\n",
       " 'input_ids': [0,\n",
       "  20263,\n",
       "  8,\n",
       "  4045,\n",
       "  126,\n",
       "  14591,\n",
       "  16,\n",
       "  372,\n",
       "  35,\n",
       "  405,\n",
       "  18,\n",
       "  8728,\n",
       "  6,\n",
       "  876,\n",
       "  5144,\n",
       "  8,\n",
       "  940,\n",
       "  4,\n",
       "  2],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'target_token_indices': [5]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "547043b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "class CustomCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.token_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Remove 'target_token_indices' temporarily\n",
    "        token_features = [{k: v for k, v in f.items() if k != 'target_token_indices'} for f in features]\n",
    "        batch = self.token_collator(token_features)\n",
    "\n",
    "        # Re-pad the target_token_indices manually\n",
    "        max_len = max(len(f[\"target_token_indices\"]) for f in features)\n",
    "        padded_indices = [\n",
    "            f[\"target_token_indices\"].tolist() + [0] * (max_len - len(f[\"target_token_indices\"]))\n",
    "            for f in features\n",
    "        ]\n",
    "        batch[\"target_token_indices\"] = torch.tensor(padded_indices)\n",
    "        return batch\n",
    "\n",
    "collator = CustomCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55e2b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AspectDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(item[\"attention_mask\"]),\n",
    "            \"target_token_indices\": torch.tensor(item[\"target_token_indices\"]),\n",
    "            \"labels\": torch.tensor(item[\"labels\"]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32140970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['polarity', 'Aspect_Category', 'Target_term', 'Character_offset', 'labels', 'Sentence', 'input_ids', 'attention_mask', 'target_token_indices'],\n",
       "    num_rows: 1503\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52afdfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/376 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 376/376 [00:00<00:00, 5204.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_val = ds_val.map(preprocess)\n",
    "ds_val  =AspectDataset(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020cfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 [Train]: 100%|██████████| 188/188 [00:19<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.6239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Validation]:   0%|          | 0/47 [00:00<?, ?it/s]/tmp/ipykernel_3260223/149379661.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"input_ids\": torch.tensor(item[\"input_ids\"]),\n",
      "/tmp/ipykernel_3260223/149379661.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(item[\"attention_mask\"]),\n",
      "/tmp/ipykernel_3260223/149379661.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"target_token_indices\": torch.tensor(item[\"target_token_indices\"]),\n",
      "/tmp/ipykernel_3260223/149379661.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"labels\": torch.tensor(item[\"labels\"]),\n",
      "Epoch 1 [Validation]: 100%|██████████| 47/47 [00:04<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Loss: 0.5972 - Val Acc: 0.7021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 188/188 [00:19<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.5492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Validation]: 100%|██████████| 47/47 [00:04<00:00,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val Loss: 0.5443 - Val Acc: 0.7633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 188/188 [00:19<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.4971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Validation]: 100%|██████████| 47/47 [00:04<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val Loss: 0.4710 - Val Acc: 0.7979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 188/188 [00:19<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.4676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Validation]: 100%|██████████| 47/47 [00:04<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val Loss: 0.5108 - Val Acc: 0.7580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 188/188 [00:19<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 0.4422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Validation]: 100%|██████████| 47/47 [00:04<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val Loss: 0.4829 - Val Acc: 0.7713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 188/188 [00:19<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 0.3887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Validation]: 100%|██████████| 47/47 [00:04<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val Loss: 0.4661 - Val Acc: 0.7527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]:  16%|█▌        | 30/188 [00:02<00:15, 10.51it/s]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "model = AspectSentimentClassifier().cuda()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_loader = DataLoader(AspectDataset(ds_train), batch_size=8, shuffle=True, collate_fn=collator)\n",
    "val_loader = DataLoader(AspectDataset(ds_val), batch_size=8, shuffle=False, collate_fn=collator)\n",
    "\n",
    "for epoch in range(20):\n",
    "    # === TRAINING ===\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "        input_ids = batch['input_ids'].cuda()\n",
    "        attention_mask = batch['attention_mask'].cuda()\n",
    "        target_token_indices = batch['target_token_indices'].cuda()\n",
    "        labels = batch['labels'].cuda().unsqueeze(1).float()\n",
    "\n",
    "        logits = model(input_ids, attention_mask, target_token_indices)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # === VALIDATION ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Validation]\"):\n",
    "            input_ids = batch['input_ids'].cuda()\n",
    "            attention_mask = batch['attention_mask'].cuda()\n",
    "            target_token_indices = batch['target_token_indices'].cuda()\n",
    "            labels = batch['labels'].cuda().unsqueeze(1).float()\n",
    "\n",
    "            logits = model(input_ids, attention_mask, target_token_indices)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "            correct += (preds == labels.bool()).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1} - Val Loss: {avg_val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24516e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
